{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success\n"
     ]
    }
   ],
   "source": [
    "#Works fine all I have to do is the following:\n",
    "\n",
    "\n",
    "from bs4 import BeautifulSoup as BS\n",
    "import requests\n",
    "import pandas as pd\n",
    "import csv\n",
    "\n",
    "\n",
    "csv_file = open('ethosuximide.csv', 'w')#1. Change the name of this file to match the drug name.\n",
    "csv_writer = csv.writer(csv_file)\n",
    "csv_writer.writerow(['Condition','Review Date','Reviewer Info','Comment'])\n",
    "\n",
    "\n",
    "\n",
    "pages=[]\n",
    "conditions=[]\n",
    "dates=[]\n",
    "reviewerInfos=[]\n",
    "comments=[]\n",
    "\n",
    "pages_to_scrape=2 #3. Here change this to the last page from the same step in number 2.\n",
    "\n",
    "for i in range(0,pages_to_scrape+1):\n",
    "    #2. After chnaging the drop list to all reviews and write click to inspect on nxt links. Here copy the first\n",
    "    # link but with {} with the index\n",
    "    url = ('https://www.webmd.com/drugs/drugreview-7099-ethosuximide+oral.aspx?drugid=7099&drugname=ethosuximide+oral&pageIndex={}&sortby=3&conditionFilter=-1').format(i)\n",
    "    pages.append(url)\n",
    "    agent = {\"User-Agent\":'Mozilla/5.0 (Windows NT 6.3; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/59.0.3071.115 Safari/537.36'}\n",
    "    page = requests.get(url, headers=agent)\n",
    "    soup = BS(page.content, 'lxml')\n",
    "\n",
    "    #soup = bs4(page.text, 'html.parser')\n",
    "    for userPost in soup.find_all('div', class_='userPost'):\n",
    "    \n",
    "        condition = userPost.find('div', class_='conditionInfo').text\n",
    "        conditions.append(condition)\n",
    "        #print(condition)\n",
    "    \n",
    "        date = userPost.find('div', class_='date').text\n",
    "        dates.append(date)\n",
    "        #print(date)\n",
    "    \n",
    "        reviewerInfo = userPost.find('p', class_='reviewerInfo').text\n",
    "        reviewerInfos.append(reviewerInfo)\n",
    "\n",
    "        #print(reviewerInfo)//done\n",
    "    \n",
    "        comment = userPost.find_all('p',class_='comment')\n",
    "        fullcomment = comment[1].text\n",
    "        comments.append(fullcomment)\n",
    "        #print(fullcomment)//done\n",
    "        csv_writer.writerow([condition, date, reviewerInfo,fullcomment])\n",
    "print(\"Success\")\n",
    "csv_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
